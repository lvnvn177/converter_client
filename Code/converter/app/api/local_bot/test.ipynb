{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wsl22/caps/003 Code/converter/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "embedding_function =\\\n",
    "    embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "\n",
    "    model_name=\"BAAI/bge-m3\", trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from uuid import uuid4\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def add_to_collection(\n",
    "        collection: chromadb.Collection, path: str\n",
    "):\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "    tmp = {\n",
    "    'page_content': [],\n",
    "    'metadata': []\n",
    "    }\n",
    "\n",
    "    for chunk in chunks:\n",
    "        tmp['page_content'].append(chunk.page_content)\n",
    "        tmp['metadata'].append(chunk.metadata)\n",
    "\n",
    "    collection.add(\n",
    "        documents=tmp['page_content'],\n",
    "        ids=[str(uuid4()) for _ in range(len(chunks))],\n",
    "        metadatas=tmp['metadata'],\n",
    "    )\n",
    "    print(\"Documents loaded to DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"test_db/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "\n",
    "\n",
    "def get_db_collection(path: str) -> chromadb.Collection:\n",
    "    try:\n",
    "        collection = client.get_collection(\n",
    "            name=os.path.basename(path),\n",
    "            embedding_function=embedding_function,\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        collection = client.create_collection(\n",
    "            name=os.path.basename(path),\n",
    "            embedding_function=embedding_function,\n",
    "            metadata={\"hnsw:space\": \"cosine\"},\n",
    "        )\n",
    "\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 1512.03385v1.pdf does not exist.\n"
     ]
    }
   ],
   "source": [
    "collection = get_db_collection('1512.03385v1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents loaded to DB\n"
     ]
    }
   ],
   "source": [
    "add_to_collection(collection, '1512.03385v1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context(query_result: dict):\n",
    "    context = \"\"\n",
    "    for doc in query_result.documents:\n",
    "        for i in doc:\n",
    "            context += i\n",
    "            context += \"\\n\"\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = collection.query(query_texts=\"resnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = generate_context(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'validation set (except†reported on the test set).\\nmethod top-5 err. ( test)\\nVGG [41] (ILSVRC’14) 7.32\\nGoogLeNet [44] (ILSVRC’14) 6.66\\nVGG [41] (v5) 6.8\\nPReLU-net [13] 4.94\\nBN-inception [16] 4.82\\nResNet (ILSVRC’15) 3.57\\nTable 5. Error rates (%) of ensembles . The top-5 error is on the\\ntest set of ImageNet and reported by the test server.\\nResNet reduces the top-1 error by 3.5% (Table 2), resulting\\nfrom the successfully reduced training error (Fig. 4 right vs.\\nleft). This comparison veriﬁes the effectiveness of residual\\nlearning on extremely deep systems.\\nLast, we also note that the 18-layer plain/residual nets\\nare comparably accurate (Table 2), but the 18-layer ResNet\\nconverges faster (Fig. 4 right vs. left). When the net is “not\\noverly deep” (18 layers here), the current SGD solver is still\\nable to ﬁnd good solutions to the plain net. In this case, the\\nResNet eases the optimization by providing faster conver-\\ngence at the early stage.able to ﬁnd good solutions to the plain net. In this case, the\\nResNet eases the optimization by providing faster conver-\\ngence at the early stage.\\nIdentity vs. Projection Shortcuts. We have shown that\\n3x3, 641x1, 64\\nrelu\\n1x1, 256relu\\nrelu3x3, 64\\n3x3, 64\\nrelurelu64-d 256-dFigure 5. A deeper residual function Ffor ImageNet. Left: a\\nbuilding block (on 56 ×56 feature maps) as in Fig. 3 for ResNet-\\n34. Right: a “bottleneck” building block for ResNet-50/101/152.\\nparameter-free, identity shortcuts help with training. Next\\nwe investigate projection shortcuts (Eqn.(2)). In Table 3 we\\ncompare three options: (A) zero-padding shortcuts are used\\nfor increasing dimensions, and all shortcuts are parameter-\\nfree (the same as Table 2 and Fig. 4 right); (B) projec-\\ntion shortcuts are used for increasing dimensions, and other\\nshortcuts are identity; and (C) all shortcuts are projections.\\nTable 3 shows that all three options are considerably bet-34-layer net with this 3-layer bottleneck block, resulting in\\na 50-layer ResNet (Table 1). We use option B for increasing\\ndimensions. This model has 3.8 billion FLOPs.\\n101-layer and 152-layer ResNets: We construct 101-\\nlayer and 152-layer ResNets by using more 3-layer blocks\\n(Table 1). Remarkably, although the depth is signiﬁcantly\\nincreased, the 152-layer ResNet (11.3 billion FLOPs) still\\nhaslower complexity than VGG-16/19 nets (15.3/19.6 bil-\\nlion FLOPs).\\nThe 50/101/152-layer ResNets are more accurate than\\nthe 34-layer ones by considerable margins (Table 3 and 4).\\nWe do not observe the degradation problem and thus en-\\njoy signiﬁcant accuracy gains from considerably increased\\ndepth. The beneﬁts of depth are witnessed for all evaluation\\nmetrics (Table 3 and 4).\\nComparisons with State-of-the-art Methods. In Table 4\\nwe compare with the previous best single-model results.\\nOur baseline 34-layer ResNets have achieved very compet-\\nitive accuracy. Our 152-layer ResNet has a single-modelbaseline+++ ResNet-101 COCO+07+12 85.6 90.0 89.6 87.8 80.8 76.1 89.9 89.9 89.6 75.5 90.0 80.7 89.6 90.3 89.1 88.7 65.4 88.1 85.6 89.0 86.8\\nTable 10. Detection results on the PASCAL VOC 2007 test set. The baseline is the Faster R-CNN system. The system “baseline+++”\\ninclude box reﬁnement, context, and multi-scale testing in Table 9.\\nsystem net data mAP areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv\\nbaseline VGG-16 07++12 70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5\\nbaseline ResNet-101 07++12 73.8 86.5 81.6 77.2 58.0 51.0 78.6 76.6 93.2 48.6 80.4 59.0 92.1 85.3 84.8 80.7 48.1 77.3 66.5 84.7 65.6\\nbaseline+++ ResNet-101 COCO+07++12 83.8 92.1 88.4 84.8 75.9 71.4 86.3 87.8 94.2 66.8 89.4 69.2 93.9 91.9 90.9 89.6 67.9 88.2 76.8 90.3 80.0\\nTable 11. Detection results on the PASCAL VOC 2012 test set ( http://host.robots.ox.ac.uk:8080/leaderboard/plain nets. The deep plain nets suffer from increased depth,\\nand exhibit higher training error when going deeper. This\\nphenomenon is similar to that on ImageNet (Fig. 4, left) and\\non MNIST (see [42]), suggesting that such an optimization\\ndifﬁculty is a fundamental problem.\\nFig. 6 (middle) shows the behaviors of ResNets. Also\\nsimilar to the ImageNet cases (Fig. 4, right), our ResNets\\nmanage to overcome the optimization difﬁculty and demon-\\nstrate accuracy gains when the depth increases.\\nWe further explore n= 18 that leads to a 110-layer\\nResNet. In this case, we ﬁnd that the initial learning rate\\nof 0.1 is slightly too large to start converging5. So we use\\n0.01 to warm up the training until the training error is below\\n80% (about 400 iterations), and then go back to 0.1 and con-\\ntinue training. The rest of the learning schedule is as done\\npreviously. This 110-layer network converges well (Fig. 6,\\nmiddle). It has fewer parameters than other deep and thining ResNet-101 net signiﬁcantly reduces the center-crop er-\\nror to 13.3%. This comparison demonstrates the excellent\\nperformance of our framework. With dense (fully convolu-\\ntional) and multi-scale testing, our ResNet-101 has an error\\nof 11.7% using ground truth classes. Using ResNet-101 for\\npredicting classes (4.6% top-5 classiﬁcation error, Table 4),\\nthe top-5 localization error is 14.4%.\\nThe above results are only based on the proposal network\\n(RPN) in Faster R-CNN [32]. One may use the detection\\nnetwork (Fast R-CNN [7]) in Faster R-CNN to improve the\\nresults. But we notice that on this dataset, one image usually\\ncontains a single dominate object, and the proposal regions\\nhighly overlap with each other and thus have very similar\\nRoI-pooled features. As a result, the image-centric training\\nof Fast R-CNN [7] generates samples of small variations,\\nwhich may not be desired for stochastic training. Motivated\\nby this, in our current experiment we use the original R-FLOPs 1.8×1093.6×1093.8×1097.6×10911.3×109\\nTable 1. Architectures for ImageNet. Building blocks are shown in brackets (see also Fig. 5), with the numbers of blocks stacked. Down-\\nsampling is performed by conv3 1, conv4 1, and conv5 1 with a stride of 2.\\n0 10 20 30 40 502030405060\\niter. (1e4)error (%)\\n  \\nplain-18\\nplain-34\\n0 10 20 30 40 502030405060\\niter. (1e4)error (%)\\n  \\nResNet-18\\nResNet-3418-layer34-layer\\n18-layer\\n34-layer\\nFigure 4. Training on ImageNet . Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain\\nnetworks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to\\ntheir plain counterparts.\\nplain ResNet\\n18 layers 27.94 27.88\\n34 layers 28.54 25.03\\nTable 2. Top-1 error (%, 10-crop testing) on ImageNet validation.\\nHere the ResNets have no extra parameter compared to their plain\\ncounterparts. Fig. 4 shows the training procedures.are ﬁne-tuned on the DET data. We split the validation set\\ninto two parts (val1/val2) following [8]. We ﬁne-tune the\\ndetection models using the DET training set and the val1\\nset. The val2 set is used for validation. We do not use other\\nILSVRC 2015 data. Our single model with ResNet-101 has\\n6http://host.robots.ox.ac.uk:8080/anonymous/3OJ4OJ.html ,\\nsubmitted on 2015-11-26.\\n11A. Object Detection Baselines\\nIn this section we introduce our detection method based\\non the baseline Faster R-CNN [32] system. The models are\\ninitialized by the ImageNet classiﬁcation models, and then\\nﬁne-tuned on the object detection data. We have experi-\\nmented with ResNet-50/101 at the time of the ILSVRC &\\nCOCO 2015 detection competitions.\\nUnlike VGG-16 used in [32], our ResNet has no hidden\\nfc layers. We adopt the idea of “Networks on Conv fea-\\nture maps” (NoC) [33] to address this issue. We compute\\nthe full-image shared conv feature maps using those lay-\\ners whose strides on the image are no greater than 16 pixels\\n(i.e., conv1, conv2 x, conv3 x, and conv4 x, totally 91 conv\\nlayers in ResNet-101; Table 1). We consider these layers as\\nanalogous to the 13 conv layers in VGG-16, and by doing\\nso, both ResNet and VGG-16 have conv feature maps of the\\nsame total stride (16 pixels). These layers are shared by a\\nregion proposal network (RPN, generating 300 proposals)ﬁx the BN layers mainly for reducing memory consumption\\nin Faster R-CNN training.\\nPASCAL VOC\\nFollowing [7, 32], for the PASCAL VOC 2007 testset,\\nwe use the 5k trainval images in VOC 2007 and 16k train-\\nvalimages in VOC 2012 for training (“07+12”). For the\\nPASCAL VOC 2012 testset, we use the 10k trainval +test\\nimages in VOC 2007 and 16k trainval images in VOC 2012\\nfor training (“07++12”). The hyper-parameters for train-\\ning Faster R-CNN are the same as in [32]. Table 7 shows\\nthe results. ResNet-101 improves the mAP by >3% over\\nVGG-16. This gain is solely because of the improved fea-\\ntures learned by ResNet.\\nMS COCO\\nThe MS COCO dataset [26] involves 80 object cate-\\ngories. We evaluate the PASCAL VOC metric (mAP @\\nIoU = 0.5) and the standard COCO metric (mAP @ IoU =\\n.5:.05:.95). We use the 80k images on the train set for train-\\ning and the 40k images on the val set for evaluation. Our\\ndetection system for COCO is similar to that for PASCAL'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
